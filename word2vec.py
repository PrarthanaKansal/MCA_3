# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hkmNb8T26xbPOsnnrwiqRFjqmiLxVMEe
"""

import nltk.corpus
nltk.download('abc')

abc= nltk.corpus.abc.words()
import urllib.request
import collections
import math
import os
import random
import zipfile
import datetime as dt

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.cm as cm

def build_dataset(words, n_words= 10000):
    """Process raw inputs into a dataset."""
    words = abc
    unique = 'UNK'
    count = [[unique, -1]]
    temp = n_words 
    temp = temp -1
    count.extend(collections.Counter(words).most_common(temp))
    dictionary = dict()
    
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for i in range(len(words)):
      if words[i] not in dictionary:
        index = 0  # dictionary['UNK']
        unk_count = unk_count + 1
      else:
        index = dictionary[word]
      data.append(index)
    count[0][1] = unk_count
    return data, count, dictionary


data_index = 0
# generate batch data
def generate_batch(data, num_skips, skip_window):
    global data_index
    # assert batch_size % num_skips == 0
    # assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    temp = 2*skip_window
    temp = temp + 1
    span = temp 
    buffer = collections.deque(maxlen=span)
    for i in range(temp):
        x = data[data_index]
        buffer.append(x)
        data_index = data_index + 1
        data_index = data_index % len(data)
        # data_index = (data_index + 1) % len(data)

    iters =  (batch_size // num_skips)
    for i in range(iters):
        target = skip_window  
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batchIndex = i*num_skips
            batchIndex = batchIndex + j
            batch[batchIndex] = buffer[skip_window] 
            context[batchIndex, 0] = buffer[target]  
        buffer.append(data[data_index])
        data_index = data_index + 1
        data_index = data_index % len(data)
        # data_index = (data_index + 1) % len(data)
    
    data_index = data_index + len(data)
    data_index = data_index - span
    data_index = data_index % len(data)
    # data_index = (data_index + len(data) - span) % len(data)
    return batch, context

data, count, dictionary = build_dataset(abc,10000)
reversed_dictionary = dict()
reversed_dictionary = {value : key for (key, value) in dictionary.items()}

batch_size = 128
embedding_size = 300  
skip_window = 2       
num_skips = 2         
vocabulary_size = 10000

valid_size = 20     
valid_window = 100  
valid_examples = np.random.choice(valid_window, valid_size, replace=False)
num_sampled = 64    

graph = tf.Graph()


def train_inputs():
  return tf.placeholder(tf.int32, shape=[batch_size]) 

def train_context():
  return tf.placeholder(tf.int32, shape=[batch_size, 1])

def valid_dataset():
  return tf.constant(valid_examples, dtype=tf.int32)

def train(embed):
  #variables for softmax
  array = [embedding_size, vocabulary_size]
  weights = tf.Variable(tf.truncated_normal(array,stddev=1.0 / math.sqrt(embedding_size)))
  temp = tf.zeros([vocabulary_size])
  biases = tf.Variable(temp)
  temp = tf.transpose(weights)
  temp2 = tf.transpose(embed)
  finalTemp = tf.matmul(temp, temp2)
  hidden_out = tf.transpose(finalTemp)
  hidden_out = hidden_out + biases
  return hidden_out

def getCrossEntropy(train_context):
    train_one_hot = tf.one_hot(train_context, vocabulary_size)
    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))
    return cross_entropy

def constructOptimizer(cross_entropy):
    return tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy) #My learning rate here is 1
global normalized_embeddings
def cosine_similarity(embeddings, valid_dataset):
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)
    return similarity

with graph.as_default():

  train_inputs = train_inputs()
  train_context = train_context()
  valid_dataset = valid_dataset()

  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
  embed = tf.nn.embedding_lookup(embeddings, train_inputs)
  hidden_out = train(embed)
  cross_entropy = getCrossEntropy(train_context)
  optimizer = constructOptimizer(cross_entropy)
  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
  normalized_embeddings = embeddings / norm
  similarity = cosine_similarity(embeddings, valid_dataset)

  # Add variable initializer.
  init = tf.global_variables_initializer()


def run(graph, num_steps):
    with tf.Session(graph=graph) as session:
      init.run()
      average_loss = 0
      for step in range(num_steps):
        batch_inputs, batch_context = generate_batch(data,num_skips, skip_window)
        feed_dict = dict()
        feed_dict[train_inputs] = batch_inputs
        feed_dict[train_context] = batch_context
        arr = [optimizer, cross_entropy]
        _, loss_val = session.run(arr, feed_dict=feed_dict)
        average_loss += loss_val

        if step % 10 == 0:
          if step > 0:
            average_loss /= 10
          print('Average loss at step ', step, ': ', average_loss)
          average_loss = 0

        if step % 10 == 0:
          sim = similarity.eval()
          for i in range(valid_size):
            valid_word = reversed_dictionary[valid_examples[i]]
            top_k = 8  # number of nearest neighbors
            nearestIndex = top_k+1
            nearest = (-sim[i, :]).argsort()[1:nearestIndex]
            log_str = 'Nearest to ' + valid_word + ": "
            for k in range(top_k):
              close_word = reversed_dictionary[nearest[k]]
              log_str = log_str+ close_word + ", "
            print(log_str)
        final_embeddings = normalized_embeddings.eval()
        tsne_plot_3d('Visualizing Embeddings using t-SNE', 'NLTK ABC Corpus', final_embeddings, 'plots/epoch_'+str(num_steps)+'_3d', a=0.1)
    return final_embeddings

num_steps = 50
softmax_start_time = dt.datetime.now()

final_embeddings= run(graph, num_steps=num_steps)
softmax_end_time = dt.datetime.now()

print("Softmax method took {} minutes to run 100 iterations".format((softmax_end_time-softmax_start_time).total_seconds()))

